# UMAP and t-SNE

This first part of the workshop will focus on dimension reduction using UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-Distributed Stochastic Neighbor Embedding).

UMAP (Uniform Manifold Approximation and Projection) is a dimensionality reduction technique that is commonly used for visualizing high-dimensional data in lower-dimensional spaces. It is particularly popular in machine learning and data analysis for tasks such as clustering, visualization, and feature engineering.

In linguistics it can be used to find and visualize groups of similar words, observations or participants.

Here's a brief overview of UMAP and a comparison with PCA (Principal Component Analysis):

### Key aspects of UMAP{-}

1. UMAP is a **nonlinear** dimensionality reduction technique, meaning it can capture complex relationships in the data that linear methods such as PCA may struggle with. Linear dimensionality reduction methods assume that the relationships between the variables are linear. In other words, the data can be represented as a linear combination of its features. When using a linear method, the data is projected onto a lower-dimensional subspace via a linear transformation of the original space. UMAP and t-SNE do not assume that the relationship between variables is linear (in fact, they assume that the relationship is nonlinear), meaning that the projection onto a lower dimensional space is not linear. However, both methods focuses on preserving both local and global structure.

2. UMAP adheres to the **Preservation of Neighborhoods** and aims to maintain the local structure of the data points. Points that are close in the high-dimensional space should remain close in the lower-dimensional representation.

3. UMAP is not restricted to reducing data to a fixed number of dimensions and it is thus assuming **Variable Dimensionality**. This means that UMAP (and t-SNE) allows for variable dimensionality, offering flexibility in the choice of the target dimensionality.

4. UMAP is more **robust** to noise and outliers compared to some other, linear, dimensionality reduction techniques, making it suitable for a wide range of data types. Also, while PCA works well when the majority of the variability is explained with the first few components, UMAP and t-SNE can handle data better where the variability is more spread out over more dimensions or components (or their equivalents).

5. Unlike some other nonlinear techniques, UMAP is designed to **preserve structure**  in the data not only locally but also globally, providing a more comprehensive representation that aims to retain the relationship between data points. In contrast, other multidimensionalty reduction methods aim to capture variability or variance while compromising relative similarity.

### Comparison between UMAP and PCA{-}

UMAP is **more flexible** in terms of capturing nonlinear relationships and allowing for variable dimensionality.

UMAP is **designed to better preserve** both local and global structures in the data, making it more suitable for certain types of high-dimensional data, especially when the relationships are intricate and nonlinear.

UMAP can be **computationally expensive** for large datasets, while PCA is generally more computationally efficient.

UMAP and PCA serve different purposes and are suitable for different scenarios. UMAP is a powerful tool for visualizing high-dimensional data, especially when nonlinear relationships need to be preserved, while PCA is a computationally efficient technique for capturing the principal components and explaining variance in a linear manner.

### How does UMAP work?{-}

The basic idea of UMAP is that a higher-dimensional graph is mapped onto a lower dimensional graph. During this projection, the relationship between individual points (local) as well as between  groups of points (global) is retained as well as possible (observations and clusters of observations that are close together in high-dimensional space should also be close together in low-dimensional space). 

UMAP starts by creating a distance matrix where the distance between each data point is calculated to all other data points.

Then, a similarity score is calculated for each point. The similarity score is calculated as e<sup>-(raw distance - distance to nearest neighbor) / $\sigma$</sup>. The similarity score depends on the number of neighbors (k) that are specified (this number includes the point itself!). The similarity score first uses the distance and then draws a curve or area around the points so that the total of the points position matches a similarity score that is the log<sub>2</sub>(k) - this means that the curve or area differs for each point. For example, if we have a cluster of points A, B, and C and we set k = 3 and the distance between A and B is 0.5 and the distance between A and C is 2.4 and the log<sub>2</sub>(3) is 1.6. Then, the curve will be drawn in a way that the curve is 1 high above point B and 0.6 above point C. The curve or area is not symmetric as otherwise some points would not have any neighbors in their allocated area. For example, the value of B relative to C can be (and probably is) different from the score of C relative to B. The fact that UMAP adapts or sets the curve or area for each data points allows us to guarantee that all points have exactly k neighbors in their area of influence and that all sums of similarities are log<sub>2</sub>(k) (thus rendering the distribution uniform in a mathematical sense). Other points do not matter and have a similarity score of 0 - this is important as it reduced the number of values for each data point from all data points (in the distance matrix) to k similarity scores for each point. 

In a next step, the similarity scores are scaled so that they are symmetric again (similar to averaging the similarity scores) when similarity scores differ. For example, the value of B relative to C could be 0.6 while the score of C relative to B could be 1.0. This would then result in a similarity score of 0.8 for both B relative to C and C relative to B. 

Now, UMAP generates an initial low-dimensional graph using spectral embedding. This graph will, however, not capture the global structure well. To adapt the initial graph, UMAP iteratively selects random pairs of points with a probability of being chosen based on their similarity score (higher similarity pairs are more likely to be chosen). Then, UMAP moves one point closer to the other - the choice which point is moved is random. Once it is determined which point is moved, UMAP selects a point from another cluster and moves the point in question further away from the foreign cluster point and closer to the same cluster point. 

The question now is how much the point should be moved. For this, UMAP calculates similarity scores for the same-cluster points and the point to be moved and the foreign cluster point. This similarity score is based on a fixed distribution (the t-distribution). The aim of moving the point is to maximize the low-dimensional value for similar, same cluster points and to minimize the value of foreign cluster, dissimilar points.

This is done iteratively until we arrive at a final low-dimensional graph. The structure of the final graph is determined to a large extent by the number of nearest neighbors (k) one chooses: a lower k will lead to smaller clusters and preservation of local structure while higher k will lead to larger clusters and preservation of global structure. 

### What are differences between UMAP and t-SNE?{-}

UMAP and t-SNE are very similar but have some differences. For instance, UMAP is substantively faster than t-SNE and UMAP always starts with the same low-dimensional graph (as it uses spectral embedding to generate the initial low-dimensional graph) while t-SNE generates different initial low-dimensional graphs for each run which leads to very different final low-dimensional graphs for each run! UMAP is deemed to have a better balance between retaining local and global structure than t-SNE (see the projection of a mammoth on the pair-code blogpost on this issue [here](https://pair-code.github.io/understanding-umap/)). Another difference is that t-SNE moves **each** point a little bit during each iteration while UMAP only moves one point (or a small number of points) per iteration.

## Practice{-}

We start with a very simple example and then move on to a more realistic example. The initial easy study simply aims at showing you the relevant functions.

We now activate the packages that we need for the dimensionality reduction methods.

```{r}
library(here)
library(readxl)
library(dplyr)
library(stringr)
library(umap)
library(tsne)
library(ggplot2)
```

### Example 1: iris {-}

In our first very basic example, we will use the `iris` data set. 

> This famous (Fisher's or Anderson's) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.

We simply load this pre-installed data set by calling the `data` function and add `"iris"` as its argument. We also inspect the data using the `head` (which displays the first 6 rows of the data set) and the `str` function (which summarized the structure of the data set).

```{r}
data("iris")
# inspect 
head(iris); str(iris)
```

We now process data and separate the features from the labels. As with simpler dimension reduction methods such as PCA and MDS, UMAP and t-SNE only work on numeric data. We can and will, however, add additional information again later for the visualization (this is why we save the labels).

As a tip if you are working with factors or character variables: you can convert factors and character variables into numeric dummy variables where the presence or absence of a level is indicated by `1` versus `0`.

```{r}
# extract
features <- iris %>%
  dplyr::select(-Species)
# extract labels
label <- iris %>%
  dplyr::select(Species)
# inspect
str(features); str(label)
```
This is all we need to do in terms of data processing.

#### Implement UMAP{-}

We can now implement the UMAP. UMAP has many arguments but the most important are:  
+ `x`: the data frame or matrix of features we want to reduce
+ `n_neighbors`: this the number of neighbors that are considered (k). The higher the number of neighbors, the bigger the groups and the more global structure is retained.
+ `n_components`: the number of target dimensions the complex data set is summarized into (2 means that the data will be compressed into 2 dimensions).

The argument `random_state` is simply the seed used to keep track of the random number generator during UMAP - this argument is not essential.


```{r}
umap.res <- umap(features, # the data set containing the features 
                 n_neighbors = 15, # number of nearest neighbors
                 n_components = 2, # number of target dimensions 
                 random_state = 15 # seed for random number generation used during umap
                 ) 
# inspect
str(umap.res)
```

In order to visualize the data, we only need to extract or retain information from the `layout` list item which is then converted into a data frame. Also, we add the labels that we extracted earlier.

```{r}
visdat <- umap.res[["layout"]] %>%
  as.data.frame() %>%
  dplyr::mutate(label)
# inspect
str(visdat)
```

We can now visualize the UMAP results.

```{r}
visdat %>%
  ggplot(aes(x = V1, y = V2, color = Species)) +
  geom_point() +
  theme_bw()
```

We can see that the resulting graph neatly differentiates the different species of iris based on their features with setosa being more different from versicolor and virginica on dimension 1. versicolor and virginica are differentiated neatly on dimension 2. 

We now implement the t-SNE dimension reduction.

### Implement t-SNE{-}

As we can use the data we processed for UMAP. we can start by implementing t-SNE right away. 

The `tsne` function has  
+ `x`: the data frame or matrix of features we want to reduce  
+ `k`: the number of target dimensions the complex data set is summarized into (2 means that the data will be compressed into 2 dimensions).  
+ `perplexity`: the number of neighbors used in the reduction (like `n_neighbors` in UMAP) 

```{r}
tsne.res <- tsne(features, # the data set containing the features 
                 k =2, # number of target dimensions (dimensions of the resulting embedding
                 perplexity = 15 # number of nearest neighbors
                 ) %>%
  as.data.frame() %>%
  dplyr::mutate(label)
# inspect
str(tsne.res)
```

We now visualize t-SNE results.

```{r}
tsne.res %>%
  ggplot(aes(x = V1, y = V2, color = Species)) +
  geom_point() +
  theme_bw()
```

We can see that although UMAP and t-SNE are mathematically very similar, their results can differ notably. 

We now turn to a more complex example using a real world data set.

### Example 2: Brechje's participants{-}

This data set was send in by Brechje and we can use it here to show how we can examine participants. In my own work I use methods like UMAP when I work with word embeddings to find similarity in parts of speech.

In a first step, we load the data using the `read_xlsx` function from the `readxl` package and inspect it using `head` and the `str` function.

```{r}
bdat <- readxl::read_xlsx(here::here("data", "SPR_trimmed_Martin.xlsx"))
# inspect
head(bdat); str(bdat)
```

In a next step, we select the columns that we need (only numeric variables plus the factors that can be used as labels) and separate labels from features.

```{r}
# extract features
bfeatures <- bdat[, c(1, 17, 21:50)] %>%
  # remove columns
  dplyr::select(-AoA_ENG,-SRP_ENG,-proportion_daily_ENG_use, -average_proportion_use_across_contexts_NOR) %>%
  # remove duplicates
  dplyr::distinct() %>%
  # remove column with NAs
  dplyr::filter(complete.cases(.)) %>%
  # clean Language group
  dplyr::mutate(Language_group = stringr::str_remove_all(Language_group, " ")) %>%
  dplyr::mutate(Language_group = stringr::str_remove_all(Language_group, "\\&")) 
# extract labels
blabel <- paste0(bfeatures$Language_group, "_",
                 bfeatures$Participant_Private_ID) 
# remove label column from features
bfeatures <- bfeatures %>%
  dplyr::select(-Participant_Private_ID, -Language_group) %>%
  # convert characters to numbers
  dplyr::mutate_if(is.character, as.numeric)
# inspect
head(bfeatures); str(bfeatures)
```

We can now implement UMAP. It makes sense to play around with the number of neighbors and components to see what solution makes sense.

```{r}
umap.bres <- umap(bfeatures, 
                  n_neighbors = 10, 
                  n_components = 2, 
                  random_state = 15
                  ) 
umap.bres <- umap.bres[["layout"]] %>%
  as.data.frame() %>%
  dplyr::mutate(Participant = blabel,
                LanguageGroup = stringr::str_remove_all(blabel, "_.*"))
# inspect
str(umap.bres)
```

```{r}
umap.bres %>%
  ggplot(aes(x = V1, y = V2, color = LanguageGroup)) +
  geom_point() +
  theme_bw()
```

In this case there is not too much to see but there are more similar groups of participants (but we did not include items or test condition in the analysis).


```{r}
sessionInfo()
```


